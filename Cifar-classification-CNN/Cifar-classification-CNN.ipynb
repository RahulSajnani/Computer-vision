{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s20BczQnQ4LT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Jg9bhwihQ8vd",
    "outputId": "25826e06-b247-4832-f531-02b8dcb7313b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# loading dataset\n",
    "batch_size = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.CIFAR100(root='./data', \n",
    "                                         train=True, \n",
    "                                         download=True,\n",
    "                                         transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                          batch_size=128, \n",
    "                                          shuffle=True, \n",
    "                                          )\n",
    "\n",
    "val_set = torchvision.datasets.CIFAR100(root='./data', \n",
    "                                        train=False, \n",
    "                                        download=True,\n",
    "                                        transform=transform)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set, \n",
    "                                         batch_size=128, \n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yStgAHNI0-Wi"
   },
   "source": [
    " # Network architecture\n",
    " ______\n",
    "\n",
    " Each feature extraction layer has a conv layer followed by a batchnorm layer. CIFAR100 dataset has 100 classes for classification. The experiments folder has different ipynb files corresponding to different experiments. The architecture in this ipynb contains 7 conv layer and also has a skip connection. Later, the last layer is followed by 2 fc layers with dropout layers to reduce over fitting. During training I use an adam optimizer with learning rate scheduler that divides the learning rate by 10 (gamma = 0.1) at the 10$^{th}$, 20$^{th}$ and 30$^{th}$ epoch.\n",
    "\n",
    " # Experiments conducted\n",
    " _______\n",
    "\n",
    " - ## Changed optimizer\n",
    "    - Changing the optimizer from stochastic gradient descent to adam increased the accuracy by 5 % for the same architecture on validation set (49 %). \n",
    " - ## Removed drop out layer\n",
    "    - By removing the drop out layer there is a reduction in validation accuracy as seen by the network. \n",
    "    - The accuracy on validation set reduces by 5 %.\n",
    " - ## Removed batchnorm layer\n",
    "    - Removing batch norm layer incorporates bias in the network and this reduces the validation accuracy to 39 %.\n",
    " - ## Varying fc layers\n",
    "    - Varying fc layers to 2 and 3 increases over fitting of the network. To reduce this I tried adding dropout layers.\n",
    "\n",
    "# Faster RCNN\n",
    "______\n",
    "\n",
    " - Added backbone layer to Faster RCNN and trained for multi class classification and bbox regression. The .ipynb is present in Faster RCNN folder.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0Y1u0IzRTNl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Network descriptions\n",
    "\n",
    "\n",
    "class Feature_extraction_layer(nn.Module):\n",
    "    '''\n",
    "    Defining feature extraction layer\n",
    "    '''\n",
    "    def __init__(self, in_channels = 3, out_channels = 16, is_pool = True):\n",
    "        \n",
    "        super(Feature_extraction_layer, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding = 1)\n",
    "        \n",
    "        self.norm = nn.BatchNorm2d(out_channels)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.is_pool = is_pool\n",
    "    \n",
    "    def forward(self, input_tensor, residual = None):\n",
    "        \n",
    "        # appending residual connection\n",
    "\n",
    "        if residual is None:\n",
    "            out = self.norm(self.conv1(input_tensor))\n",
    "        \n",
    "        else:\n",
    "            out = torch.cat((input_tensor, residual), dim = 1)\n",
    "            out = self.norm(self.conv1(out))\n",
    "        \n",
    "        # If is_pool true then maxpool 2D\n",
    "        if self.is_pool:\n",
    "            out = self.pool(out)\n",
    "        \n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Feature_extraction_net(nn.Module):\n",
    "    '''\n",
    "    Defining Feature extraction network\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Feature_extraction_net, self).__init__()\n",
    "        self.dont_pool = False\n",
    "        \n",
    "        self.layer1 = Feature_extraction_layer(3, 16)\n",
    "        self.layer2 = Feature_extraction_layer(16, 32, self.dont_pool)\n",
    "        \n",
    "        # layer 1 + layer 2 concat 16 + 32 = 48\n",
    "\n",
    "        self.layer3 = Feature_extraction_layer(48, 64, self.dont_pool)\n",
    "        self.layer4 = Feature_extraction_layer(64, 64)\n",
    "        self.layer5 = Feature_extraction_layer(64, 128)\n",
    "        # self.layer6 = Feature_extraction_layer(128, 256)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        out1 = self.layer1(input_tensor)\n",
    "        out2 = self.layer2(out1)\n",
    "        \n",
    "        out3 = self.layer3(out2, out1)\n",
    "        out4 = self.layer4(out3)\n",
    "        out5 = self.layer5(out4)\n",
    "        # out6 = self.layer6(out5)\n",
    "        # out7 = self.layer7(out6, out5)\n",
    "        \n",
    "        # print(out6.size())\n",
    "        return out5\n",
    "\n",
    "class CNN_network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CNN_network, self).__init__()\n",
    "\n",
    "        self.feature_extractor = Feature_extraction_net()\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.drop2 = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        # self.fc2 = nn.Linear(512, 254)\n",
    "        self.fc3 = nn.Linear(128, 100)\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        out = self.feature_extractor(input_tensor)\n",
    "        out = self.drop2(out.view( -1, 128 * 4 * 4 ))\n",
    "        out = (F.elu(self.fc1(out)))\n",
    "        # out = self.drop2(F.elu(self.fc2(out)))\n",
    "        out = self.drop2(F.elu(self.fc3(out)))\n",
    "        # print(out.size())\n",
    "        return out\n",
    "\n",
    "def correct_prediction( prediction, labels):\n",
    "\n",
    "    _, index = torch.max(prediction, 1)\n",
    "    correct = torch.eq(labels, index)\n",
    "    total_correct = torch.sum(correct).item()\n",
    "    return total_correct\n",
    "\n",
    "def train_network(net, nepochs, lr_scheduler, train_loader, val_loader, optimizer, criterion):\n",
    "    '''\n",
    "    Function to train the network given different loaders\n",
    "    '''\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    num_train_samples = len(train_loader.dataset) \n",
    "    num_val_samples = len(val_loader.dataset)\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "\n",
    "        running_loss_train = 0.0\n",
    "        correct_train = 0;\n",
    "        net.train()\n",
    "        \n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "          \n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            # print(labels.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss_train += loss.item()\n",
    "            correct_train += correct_prediction(outputs, labels) \n",
    "\n",
    "        mean_loss = running_loss_train / num_train_samples\n",
    "        accuracy_train = round((correct_train / num_train_samples)*100,2)\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "        \n",
    "            print(\"Epoch \", epoch, \" train loss \", mean_loss)\n",
    "            print(\"Train accuracy: \", accuracy_train,\"%\" )\n",
    "                \n",
    "        \n",
    "        train_acc.append(accuracy_train)\n",
    "        train_loss.append(mean_loss)\n",
    "\n",
    "        # check for val set\n",
    "        net.eval()\n",
    "        running_loss_val = 0\n",
    "        correct_val = 0\n",
    "\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "          \n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            # print(labels.size())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss_val += loss.item()\n",
    "            correct_val += correct_prediction(outputs, labels) \n",
    "\n",
    "        mean_loss_val = running_loss_val / num_val_samples\n",
    "        accuracy_val = round((correct_val / num_val_samples)*100,2)\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Epoch \", epoch, \" validation loss \", mean_loss_val)\n",
    "            print(\"Validation accuracy: \", accuracy_val ,\"%\" )\n",
    "\n",
    "        val_acc.append(accuracy_val)\n",
    "        val_loss.append(mean_loss_val)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "def weights_init(net):\n",
    "    if isinstance(net, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(net.weight.data)\n",
    "        nn.init.xavier_normal_(net.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jh_D5OvYURN2",
    "outputId": "d21a7aaa-dba7-4976-e235-518e819fda3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu\n",
      "Epoch  0  train loss  0.02842044912338257\n",
      "Train accuracy:  16.94 %\n",
      "Epoch  0  validation loss  0.02342667770385742\n",
      "Validation accuracy:  27.61 %\n",
      "Epoch  2  train loss  0.020096738901138306\n",
      "Train accuracy:  38.31 %\n",
      "Epoch  2  validation loss  0.019077400922775267\n",
      "Validation accuracy:  39.12 %\n",
      "Epoch  4  train loss  0.016921128940582276\n",
      "Train accuracy:  48.05 %\n",
      "Epoch  4  validation loss  0.01712595282793045\n",
      "Validation accuracy:  43.56 %\n",
      "Epoch  6  train loss  0.014975869448184968\n",
      "Train accuracy:  54.31 %\n",
      "Epoch  6  validation loss  0.01595147725343704\n",
      "Validation accuracy:  48.81 %\n",
      "Epoch  8  train loss  0.013350130441188813\n",
      "Train accuracy:  59.7 %\n",
      "Epoch  8  validation loss  0.015801086938381195\n",
      "Validation accuracy:  49.03 %\n",
      "Epoch  10  train loss  0.010171977208852767\n",
      "Train accuracy:  71.11 %\n",
      "Epoch  10  validation loss  0.014444028282165528\n",
      "Validation accuracy:  54.21 %\n",
      "Epoch  12  train loss  0.009411103267669677\n",
      "Train accuracy:  73.78 %\n",
      "Epoch  12  validation loss  0.014407230508327484\n",
      "Validation accuracy:  54.57 %\n",
      "Epoch  14  train loss  0.009020447344779968\n",
      "Train accuracy:  75.12 %\n",
      "Epoch  14  validation loss  0.014500460064411164\n",
      "Validation accuracy:  54.28 %\n",
      "Epoch  16  train loss  0.00880847389817238\n",
      "Train accuracy:  75.75 %\n",
      "Epoch  16  validation loss  0.014597685217857361\n",
      "Validation accuracy:  54.2 %\n",
      "Epoch  18  train loss  0.008511114048957824\n",
      "Train accuracy:  76.86 %\n",
      "Epoch  18  validation loss  0.014639568984508515\n",
      "Validation accuracy:  54.49 %\n",
      "Epoch  20  train loss  0.00810108952999115\n",
      "Train accuracy:  78.52 %\n",
      "Epoch  20  validation loss  0.01466191532611847\n",
      "Validation accuracy:  54.58 %\n",
      "Epoch  22  train loss  0.008019940254688263\n",
      "Train accuracy:  78.92 %\n",
      "Epoch  22  validation loss  0.01468782147169113\n",
      "Validation accuracy:  54.53 %\n",
      "Epoch  24  train loss  0.008005425698757172\n",
      "Train accuracy:  78.78 %\n",
      "Epoch  24  validation loss  0.014695379221439361\n",
      "Validation accuracy:  54.61 %\n",
      "Epoch  26  train loss  0.007988755388259888\n",
      "Train accuracy:  79.04 %\n",
      "Epoch  26  validation loss  0.014696149039268493\n",
      "Validation accuracy:  54.39 %\n",
      "Epoch  28  train loss  0.007899633564949035\n",
      "Train accuracy:  79.28 %\n",
      "Epoch  28  validation loss  0.014711765122413635\n",
      "Validation accuracy:  54.51 %\n",
      "Epoch  30  train loss  0.007874906394481658\n",
      "Train accuracy:  79.22 %\n",
      "Epoch  30  validation loss  0.014715794122219086\n",
      "Validation accuracy:  54.32 %\n",
      "Epoch  32  train loss  0.007899708603620529\n",
      "Train accuracy:  79.17 %\n",
      "Epoch  32  validation loss  0.014718990850448609\n",
      "Validation accuracy:  54.44 %\n",
      "Epoch  34  train loss  0.00785299271941185\n",
      "Train accuracy:  79.49 %\n",
      "Epoch  34  validation loss  0.014725467026233672\n",
      "Validation accuracy:  54.52 %\n",
      "Epoch  36  train loss  0.007849618619680405\n",
      "Train accuracy:  79.42 %\n",
      "Epoch  36  validation loss  0.014724742782115937\n",
      "Validation accuracy:  54.39 %\n",
      "Epoch  38  train loss  0.00783210551738739\n",
      "Train accuracy:  79.28 %\n",
      "Epoch  38  validation loss  0.014722104334831237\n",
      "Validation accuracy:  54.43 %\n",
      "Epoch  40  train loss  0.007961145414113998\n",
      "Train accuracy:  79.21 %\n",
      "Epoch  40  validation loss  0.01471559110879898\n",
      "Validation accuracy:  54.45 %\n",
      "Epoch  42  train loss  0.007860582385063172\n",
      "Train accuracy:  79.36 %\n",
      "Epoch  42  validation loss  0.014721420311927795\n",
      "Validation accuracy:  54.43 %\n",
      "Epoch  44  train loss  0.007858962697982788\n",
      "Train accuracy:  79.32 %\n",
      "Epoch  44  validation loss  0.014726763260364532\n",
      "Validation accuracy:  54.68 %\n",
      "Epoch  46  train loss  0.007803196296691894\n",
      "Train accuracy:  79.42 %\n",
      "Epoch  46  validation loss  0.014730555117130279\n",
      "Validation accuracy:  54.45 %\n",
      "Epoch  48  train loss  0.007913544460535049\n",
      "Train accuracy:  79.24 %\n",
      "Epoch  48  validation loss  0.014735991954803468\n",
      "Validation accuracy:  54.5 %\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    \n",
    "    net = CNN_network()\n",
    "    # net.apply(weights_init)\n",
    "    use_cuda = True\n",
    "\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "        print('using gpu')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = 1e-3\n",
    "    momentum = 0.99\n",
    "    nepochs = 100\n",
    "    optimizer = optim.Adam(net.parameters(), lr)\n",
    "    \n",
    "    # lr_scheduler\n",
    "    lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [10, 20, 30], gamma=0.1)\n",
    "\n",
    "    train_network(net, 50, lr_scheduler, train_loader, val_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FcQ2Ch9fNdq"
   },
   "outputs": [],
   "source": [
    " from google.colab import files\n",
    "\n",
    "torch.save(net.state_dict(), 'checkpointcv_4_xavier.pth')\n",
    "\n",
    "# download checkpoint file\n",
    "files.download('checkpointcv_4_xavier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJpwSy68zcbu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "20171056_Assignment_4_classification_optimizer_adam_add_dropout_xavier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
